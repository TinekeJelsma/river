{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The art of using pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems.\n",
    "\n",
    "Both [scikit-learn](https://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline) and [pandas](https://tomaugspurger.github.io/method-chaining) make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's `pipeline` module, however the `pipe` method from `pandas` is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations.\n",
    "\n",
    "Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not!\n",
    "\n",
    "In this notebook we'll manipulate data from the [Kaggle Recruit Restaurants Visitor Forecasting competition](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting). The data is directly available through `river`'s `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:37:09.594474Z",
     "iopub.status.busy": "2020-11-20T00:37:09.593538Z",
     "iopub.status.idle": "2020-11-20T00:37:11.160129Z",
     "shell.execute_reply": "2020-11-20T00:37:11.159614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip (4.28 MB)\n",
      "Uncompressing into /home/travis/river_data/Restaurants\n",
      "{'area_name': 'Tōkyō-to Nerima-ku Toyotamakita',\n",
      " 'date': datetime.datetime(2016, 1, 1, 0, 0),\n",
      " 'genre_name': 'Izakaya',\n",
      " 'is_holiday': True,\n",
      " 'latitude': 35.7356234,\n",
      " 'longitude': 139.6516577,\n",
      " 'store_id': 'air_04341b588bde96cd'}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from river import datasets\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    pprint(x)\n",
    "    pprint(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:37:11.205925Z",
     "iopub.status.busy": "2020-11-20T00:37:11.169916Z",
     "iopub.status.idle": "2020-11-20T00:37:34.318575Z",
     "shell.execute_reply": "2020-11-20T00:37:34.319026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.465114\n"
     ]
    }
   ],
   "source": [
    "from river import feature_extraction\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river import stats\n",
    "\n",
    "\n",
    "means = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    \n",
    "    # Derive date features\n",
    "    x['weekday'] = x['date'].weekday()\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "    \n",
    "    # Process the rolling means of the target  \n",
    "    for mean in means:\n",
    "        x = {**x, **mean.transform_one(x)}\n",
    "        mean.learn_one(x, y)\n",
    "    \n",
    "    # Remove the key/value pairs that aren't features\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "    \n",
    "    # Rescale the data\n",
    "    x = scaler.learn_one(x).transform_one(x)\n",
    "    \n",
    "    # Fit the linear regression\n",
    "    y_pred = lin_reg.predict_one(x)\n",
    "    lin_reg.learn_one(x, y)\n",
    "    \n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "    \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using many features. We can print the last `x` to get an idea of the features (don't forget they've been scaled!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:37:34.326456Z",
     "iopub.status.busy": "2020-11-20T00:37:34.324908Z",
     "iopub.status.idle": "2020-11-20T00:37:34.328278Z",
     "shell.execute_reply": "2020-11-20T00:37:34.327836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_holiday': -0.23103573677646685,\n",
      " 'is_weekend': 1.6249280076334165,\n",
      " 'target_rollingmean_14_by_store_id': -1.4125913815779154,\n",
      " 'target_rollingmean_21_by_store_id': -1.3980979075298519,\n",
      " 'target_rollingmean_7_by_store_id': -1.3502314499809096,\n",
      " 'weekday': 1.0292832579142892}\n"
     ]
    }
   ],
   "source": [
    "pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as `river` is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline *à la sklearn*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:37:34.337561Z",
     "iopub.status.busy": "2020-11-20T00:37:34.336680Z",
     "iopub.status.idle": "2020-11-20T00:38:10.276860Z",
     "shell.execute_reply": "2020-11-20T00:38:10.277282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.38533\n"
     ]
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    \n",
    "    # Make a prediction without using the target\n",
    "    y_pred = model.predict_one(x)\n",
    "    \n",
    "    # Update the model using the target\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "    \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Pipeline` to arrange each step in a sequential order. A `TransformerUnion` is used to merge multiple feature extractors into a single transformer. The `for` loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called `progressive_val_score` part of the `evaluate` module. We can use it to replace the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:38:10.301938Z",
     "iopub.status.busy": "2020-11-20T00:38:10.286203Z",
     "iopub.status.idle": "2020-11-20T00:38:53.991582Z",
     "shell.execute_reply": "2020-11-20T00:38:53.992023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=datasets.Restaurants(), model=model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you couldn't have used the `progressive_val_score` method if you wrote the model in a procedural manner.\n",
    "\n",
    "Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However `river` has some special tricks up it's sleeve to save you from a lot of pain.\n",
    "\n",
    "The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then `river` automatically infers one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:38:54.023789Z",
     "iopub.status.busy": "2020-11-20T00:38:53.999971Z",
     "iopub.status.idle": "2020-11-20T00:39:37.889963Z",
     "shell.execute_reply": "2020-11-20T00:39:37.889419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood a `Pipeline` inherits from `collections.OrderedDict`. Indeed this makes sense because if you think about it a `Pipeline` is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a `Pipeline` the same way you would manipulate an ordinary `dict`. For instance we can print the name of each step by using the `keys` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:39:37.896098Z",
     "iopub.status.busy": "2020-11-20T00:39:37.894599Z",
     "iopub.status.idle": "2020-11-20T00:39:37.897962Z",
     "shell.execute_reply": "2020-11-20T00:39:37.897483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerUnion\n",
      "Discard\n",
      "StandardScaler\n",
      "LinearRegression\n"
     ]
    }
   ],
   "source": [
    "for name in model.steps:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is a `FeatureUnion` and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious.\n",
    "\n",
    "The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the `+` operator to merge `Transformer`s into a `TransformerUnion`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:39:37.906179Z",
     "iopub.status.busy": "2020-11-20T00:39:37.905260Z",
     "iopub.status.idle": "2020-11-20T00:40:21.648416Z",
     "shell.execute_reply": "2020-11-20T00:40:21.647952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewhise we can use the `|` operator to assemble steps into a `Pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:40:21.657434Z",
     "iopub.status.busy": "2020-11-20T00:40:21.656533Z",
     "iopub.status.idle": "2020-11-20T00:41:05.177158Z",
     "shell.execute_reply": "2020-11-20T00:41:05.176672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "model = model | compose.Discard(*to_discard) | preprocessing.StandardScaler()\n",
    "\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a `FuncTransformer`, which can be quite handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:41:05.195647Z",
     "iopub.status.busy": "2020-11-20T00:41:05.184154Z",
     "iopub.status.idle": "2020-11-20T00:41:48.776313Z",
     "shell.execute_reply": "2020-11-20T00:41:48.776964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(n))\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own!\n",
    "\n",
    "Before finishing we can take a look at what our pipeline looks graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T00:41:48.781745Z",
     "iopub.status.busy": "2020-11-20T00:41:48.781183Z",
     "iopub.status.idle": "2020-11-20T00:41:48.819698Z",
     "shell.execute_reply": "2020-11-20T00:41:48.820339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1290pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 1289.73 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-400 1285.73,-400 1285.73,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\"><title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"557.892\" cy=\"-378\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"557.892\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- get_date_features -->\n",
       "<g id=\"node2\" class=\"node\"><title>get_date_features</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"94.8916\" cy=\"-306\" rx=\"94.7833\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.8916\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_date_features</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;get_date_features -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>x&#45;&gt;get_date_features</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M531.745,-373.281C472.48,-364.821 323.398,-343.365 198.892,-324 190.771,-322.737 182.297,-321.394 173.846,-320.037\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.386,-316.579 163.956,-318.443 173.272,-323.49 174.386,-316.579\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_7_by_store_id -->\n",
       "<g id=\"node3\" class=\"node\"><title>target_rollingmean_7_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"376.892\" cy=\"-306\" rx=\"168.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"376.892\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">target_rollingmean_7_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_7_by_store_id -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>x&#45;&gt;target_rollingmean_7_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M534.968,-368.134C508.535,-357.912 464.232,-340.778 429.242,-327.246\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"430.232,-323.876 419.643,-323.534 427.707,-330.405 430.232,-323.876\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_14_by_store_id -->\n",
       "<g id=\"node4\" class=\"node\"><title>target_rollingmean_14_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"738.892\" cy=\"-306\" rx=\"174.669\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"738.892\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">target_rollingmean_14_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_14_by_store_id -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>x&#45;&gt;target_rollingmean_14_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M580.815,-368.134C607.249,-357.912 651.551,-340.778 686.541,-327.246\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"688.076,-330.405 696.14,-323.534 685.551,-323.876 688.076,-330.405\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_21_by_store_id -->\n",
       "<g id=\"node5\" class=\"node\"><title>target_rollingmean_21_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1106.89\" cy=\"-306\" rx=\"174.669\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1106.89\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">target_rollingmean_21_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_21_by_store_id -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>x&#45;&gt;target_rollingmean_21_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M584.371,-373.624C656.703,-364.401 860.935,-338.361 991.604,-321.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"992.186,-325.154 1001.66,-320.417 991.301,-318.21 992.186,-325.154\"/>\n",
       "</g>\n",
       "<!-- ~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"node6\" class=\"node\"><title>~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"557.892\" cy=\"-234\" rx=\"329.542\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"557.892\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</text>\n",
       "</g>\n",
       "<!-- get_date_features&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>get_date_features&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.956,-293.557C175.612,-291.67 187.593,-289.757 198.892,-288 277.956,-275.703 366.93,-262.562 436.521,-252.442\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"437.252,-255.873 446.644,-250.971 436.245,-248.946 437.252,-255.873\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_7_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>target_rollingmean_7_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M419.786,-288.411C445.111,-278.617 477.388,-266.134 504.369,-255.699\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.983,-258.828 514.048,-251.956 503.458,-252.299 505.983,-258.828\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_14_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>target_rollingmean_14_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M695.997,-288.411C670.672,-278.617 638.395,-266.134 611.414,-255.699\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"612.325,-252.299 601.735,-251.956 609.8,-258.828 612.325,-252.299\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_21_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>target_rollingmean_21_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1001.69,-291.586C913.43,-280.333 786.865,-264.195 691.463,-252.031\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"691.816,-248.548 681.454,-250.755 690.931,-255.492 691.816,-248.548\"/>\n",
       "</g>\n",
       "<!-- StandardScaler -->\n",
       "<g id=\"node7\" class=\"node\"><title>StandardScaler</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"557.892\" cy=\"-162\" rx=\"82.5854\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"557.892\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">StandardScaler</text>\n",
       "</g>\n",
       "<!-- ~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]&#45;&gt;StandardScaler -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]&#45;&gt;StandardScaler</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M557.892,-215.697C557.892,-207.983 557.892,-198.712 557.892,-190.112\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"561.392,-190.104 557.892,-180.104 554.392,-190.104 561.392,-190.104\"/>\n",
       "</g>\n",
       "<!-- LinearRegression -->\n",
       "<g id=\"node8\" class=\"node\"><title>LinearRegression</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"557.892\" cy=\"-90\" rx=\"92.8835\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"557.892\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">LinearRegression</text>\n",
       "</g>\n",
       "<!-- StandardScaler&#45;&gt;LinearRegression -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>StandardScaler&#45;&gt;LinearRegression</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M557.892,-143.697C557.892,-135.983 557.892,-126.712 557.892,-118.112\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"561.392,-118.104 557.892,-108.104 554.392,-118.104 561.392,-118.104\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node9\" class=\"node\"><title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"557.892\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"557.892\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- LinearRegression&#45;&gt;y -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>LinearRegression&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M557.892,-71.6966C557.892,-63.9827 557.892,-54.7125 557.892,-46.1124\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"561.392,-46.1043 557.892,-36.1043 554.392,-46.1044 561.392,-46.1043\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f83526fa7b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
